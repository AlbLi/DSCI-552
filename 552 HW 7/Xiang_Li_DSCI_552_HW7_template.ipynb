{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b0dde94",
   "metadata": {},
   "source": [
    "## Author: Xiang (Albert) Li\n",
    "## USC ID: 1892796881\n",
    "## Github Userid: XiangLi1209\n",
    "## Created Time: Apr 3rd, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647df918",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a471878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import sklearn.metrics\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import itertools\n",
    "import copy\n",
    "import os\n",
    "import shutil\n",
    "import urllib.request\n",
    "import csv\n",
    "import pandas as pd\n",
    "from scipy.stats import bootstrap\n",
    "import statistics\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "import os\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a2d4b9",
   "metadata": {},
   "source": [
    "## 1. Multi-class and Multi-Label Classi cation Using Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51461395",
   "metadata": {},
   "source": [
    "#### (a) Download the Anuran Calls (MFCCs) Data Set from: https://archive.ics.uci.edu/ml/datasets/Anuran+Calls+%28MFCCs%29. Choose 70% of the data randomly as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f848318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92f8ce07",
   "metadata": {},
   "source": [
    "#### (b) Each instance has three labels: Families, Genus, and Species. Each of the labels has multiple classes. We wish to solve a multi-class and multi-label problem.\n",
    "#### One of the most important approaches to multi-label classi cation is to train a classifier for each label (binary relevance). We  first try this approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc1ce0f",
   "metadata": {},
   "source": [
    "#### i. Research exact match and hamming score/ loss methods for evaluating multi-label classification and use them in evaluating the classifiers in this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d1108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d35c643",
   "metadata": {},
   "source": [
    "#### ii. Train a SVM for each of the labels, using Gaussian kernels and one versus all classifiers. Determine the weight of the SVM penalty and the width of the Gaussian Kernel using 10 fold cross validation.[<sup>1</sup>](#fn1) You are welcome to try to solve the problem with both standardized [<sup>2</sup>](#fn2) and raw attributes and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44b243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4e55506",
   "metadata": {},
   "source": [
    "#### iii. Repeat 1(b)ii with L1-penalized SVMs.[<sup>3</sup>](#fn3) Remember to standardize[<sup>4</sup>](#fn4) the attributes. Determine the weight of the SVM penalty using 10 fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850eac7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86905e6a",
   "metadata": {},
   "source": [
    "#### iv. Repeat 1(b)iii by using SMOTE or any other method you know to remedy class imbalance. Report your conclusions about the classifiers you trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00f0e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d0453e1",
   "metadata": {},
   "source": [
    "#### v. Extra Practice: Study the Classifier Chain method and apply it to the above problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae59c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04a0579b",
   "metadata": {},
   "source": [
    "#### vi. Extra Practice: Research how confusion matrices, precision, recall, ROC, and AUC are defined for multi-label classification and compute them for the classifiers you trained in above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f353e0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "772798cc",
   "metadata": {},
   "source": [
    "### 2. K-Means Clustering on a Multi-Class and Multi-Label Data Set Monte-Carlo Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c574cccb",
   "metadata": {},
   "source": [
    "#### Perform the following procedures 50 times, and report the average and standard deviation of the 50 Hamming Distances that you calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fbf511",
   "metadata": {},
   "source": [
    "#### (a) Use k-means clustering on the whole Anuran Calls (MFCCs) Data Set (do not split the data into train and test, as we are not performing supervised learning in this exercise). Choose $k \\in \\{1,2,......,50\\}$ automatically based on one of the methods provided in the slides (CH or Gap Statistics or scree plots or Silhouettes) or any other method you know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d910c413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67ee2dae",
   "metadata": {},
   "source": [
    "#### (b) In each cluster, determine which family is the majority by reading the true labels. Repeat for genus and species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94380110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6d7a9b3",
   "metadata": {},
   "source": [
    "#### (c) Now for each cluster you have a majority label triplet (family, genus, species). Calculate the average Hamming distance, Hamming score, and Hamming loss[<sup>5</sup>](#fn5) between the true labels and the labels assigned by clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171b283a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98696b11",
   "metadata": {},
   "source": [
    "### 3. ISLR 12.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d528a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ed15945",
   "metadata": {},
   "source": [
    "##### <span style='color:green '> Footnote section <span>\n",
    "\n",
    "<span id=\"fn1\">1.How to choose parameter ranges for SVMs? One can use wide ranges for the parameters and a  fine grid (e.g. 1000 points) for cross validation; however,this method may be computationally expensive. An alternative way is to train the SVM with very large and very small parameters on the whole training data and find very large and very small parameters for which the training accuracy is not below a threshold (e.g.,70%). Then one can select a  fixed number of parameters (e.g., 20) between those points for cross validation. For the penalty parameter, usually one has to consider increments in log( $\\lambda$). For example, if one found that the accuracy of a support vector machine will not be below 70% for $\\lambda$ = $10^{-3}$ and $\\lambda$  = $10^{6}$, one has to choose log($\\lambda$ ) $ \\in \\{-3,-2,......,4,5,6\\}$.   \n",
    "    For the Gaussian Kernel parameter, one usually chooses linear increments, e.g. $\\sigma \\in\\{.1,.2,....,2\\}$. When both $\\sigma$  and $\\lambda$ are to be chosen using cross-validation, combinations of very\n",
    "small and very large $\\sigma$'s  and $\\lambda$'s that keep the accuracy above a threshold (e.g.70%) can be used to determine the ranges for   $\\sigma$  and $\\lambda$. Please note that these are very rough rules of thumb, not general procedures. <span>  \n",
    "<span id=\"fn2\"> 2. It seems that the data are already normalized. <span>  \n",
    "<span id=\"fn3\"> 3. The convention is to use L1 penalty with linear kernel.<span>  \n",
    "<span id=\"fn4\"> 4. It seems that the data are already normalized. <span>   \n",
    "<span id=\"fn5\"> 5.Research what these scores are. For example, see the paper A Literature Survey on Algorithms for Multi-label Learning, by Mohammad Sorower."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
