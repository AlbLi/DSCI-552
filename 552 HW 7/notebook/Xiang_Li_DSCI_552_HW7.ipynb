{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b0dde94",
   "metadata": {},
   "source": [
    "## Author: Xiang (Albert) Li\n",
    "## USC ID: 1892796881\n",
    "## Github Userid: XiangLi1209\n",
    "## Created Time: Apr 3rd, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647df918",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af78d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import sklearn.metrics\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import itertools\n",
    "import copy\n",
    "import os\n",
    "import shutil\n",
    "import urllib.request\n",
    "import csv\n",
    "import pandas as pd\n",
    "from scipy.stats import bootstrap\n",
    "import statistics\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, hamming_loss,silhouette_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "import os\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from datetime import datetime\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import hamming_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f45b958",
   "metadata": {},
   "source": [
    "## 1. Multi-class and Multi-Label Classi cation Using Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca6dea5",
   "metadata": {},
   "source": [
    "### (a) Download the Anuran Calls (MFCCs) Data Set from: https://archive.ics.uci.edu/ml/datasets/Anuran+Calls+%28MFCCs%29. Choose 70% of the data randomly as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4faedb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MFCCs_ 1</th>\n",
       "      <th>MFCCs_ 2</th>\n",
       "      <th>MFCCs_ 3</th>\n",
       "      <th>MFCCs_ 4</th>\n",
       "      <th>MFCCs_ 5</th>\n",
       "      <th>MFCCs_ 6</th>\n",
       "      <th>MFCCs_ 7</th>\n",
       "      <th>MFCCs_ 8</th>\n",
       "      <th>MFCCs_ 9</th>\n",
       "      <th>MFCCs_10</th>\n",
       "      <th>...</th>\n",
       "      <th>MFCCs_17</th>\n",
       "      <th>MFCCs_18</th>\n",
       "      <th>MFCCs_19</th>\n",
       "      <th>MFCCs_20</th>\n",
       "      <th>MFCCs_21</th>\n",
       "      <th>MFCCs_22</th>\n",
       "      <th>Family</th>\n",
       "      <th>Genus</th>\n",
       "      <th>Species</th>\n",
       "      <th>RecordID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.152936</td>\n",
       "      <td>-0.105586</td>\n",
       "      <td>0.200722</td>\n",
       "      <td>0.317201</td>\n",
       "      <td>0.260764</td>\n",
       "      <td>0.100945</td>\n",
       "      <td>-0.150063</td>\n",
       "      <td>-0.171128</td>\n",
       "      <td>0.124676</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108351</td>\n",
       "      <td>-0.077623</td>\n",
       "      <td>-0.009568</td>\n",
       "      <td>0.057684</td>\n",
       "      <td>0.118680</td>\n",
       "      <td>0.014038</td>\n",
       "      <td>Leptodactylidae</td>\n",
       "      <td>Adenomera</td>\n",
       "      <td>AdenomeraAndre</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.171534</td>\n",
       "      <td>-0.098975</td>\n",
       "      <td>0.268425</td>\n",
       "      <td>0.338672</td>\n",
       "      <td>0.268353</td>\n",
       "      <td>0.060835</td>\n",
       "      <td>-0.222475</td>\n",
       "      <td>-0.207693</td>\n",
       "      <td>0.170883</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090974</td>\n",
       "      <td>-0.056510</td>\n",
       "      <td>-0.035303</td>\n",
       "      <td>0.020140</td>\n",
       "      <td>0.082263</td>\n",
       "      <td>0.029056</td>\n",
       "      <td>Leptodactylidae</td>\n",
       "      <td>Adenomera</td>\n",
       "      <td>AdenomeraAndre</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.152317</td>\n",
       "      <td>-0.082973</td>\n",
       "      <td>0.287128</td>\n",
       "      <td>0.276014</td>\n",
       "      <td>0.189867</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>-0.242234</td>\n",
       "      <td>-0.219153</td>\n",
       "      <td>0.232538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050691</td>\n",
       "      <td>-0.023590</td>\n",
       "      <td>-0.066722</td>\n",
       "      <td>-0.025083</td>\n",
       "      <td>0.099108</td>\n",
       "      <td>0.077162</td>\n",
       "      <td>Leptodactylidae</td>\n",
       "      <td>Adenomera</td>\n",
       "      <td>AdenomeraAndre</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.224392</td>\n",
       "      <td>0.118985</td>\n",
       "      <td>0.329432</td>\n",
       "      <td>0.372088</td>\n",
       "      <td>0.361005</td>\n",
       "      <td>0.015501</td>\n",
       "      <td>-0.194347</td>\n",
       "      <td>-0.098181</td>\n",
       "      <td>0.270375</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136009</td>\n",
       "      <td>-0.177037</td>\n",
       "      <td>-0.130498</td>\n",
       "      <td>-0.054766</td>\n",
       "      <td>-0.018691</td>\n",
       "      <td>0.023954</td>\n",
       "      <td>Leptodactylidae</td>\n",
       "      <td>Adenomera</td>\n",
       "      <td>AdenomeraAndre</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.087817</td>\n",
       "      <td>-0.068345</td>\n",
       "      <td>0.306967</td>\n",
       "      <td>0.330923</td>\n",
       "      <td>0.249144</td>\n",
       "      <td>0.006884</td>\n",
       "      <td>-0.265423</td>\n",
       "      <td>-0.172700</td>\n",
       "      <td>0.266434</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048885</td>\n",
       "      <td>-0.053074</td>\n",
       "      <td>-0.088550</td>\n",
       "      <td>-0.031346</td>\n",
       "      <td>0.108610</td>\n",
       "      <td>0.079244</td>\n",
       "      <td>Leptodactylidae</td>\n",
       "      <td>Adenomera</td>\n",
       "      <td>AdenomeraAndre</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7190</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.554504</td>\n",
       "      <td>-0.337717</td>\n",
       "      <td>0.035533</td>\n",
       "      <td>0.034511</td>\n",
       "      <td>0.443451</td>\n",
       "      <td>0.093889</td>\n",
       "      <td>-0.100753</td>\n",
       "      <td>0.037087</td>\n",
       "      <td>0.081075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069430</td>\n",
       "      <td>0.071001</td>\n",
       "      <td>0.021591</td>\n",
       "      <td>0.052449</td>\n",
       "      <td>-0.021860</td>\n",
       "      <td>-0.079860</td>\n",
       "      <td>Hylidae</td>\n",
       "      <td>Scinax</td>\n",
       "      <td>ScinaxRuber</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7191</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.517273</td>\n",
       "      <td>-0.370574</td>\n",
       "      <td>0.030673</td>\n",
       "      <td>0.068097</td>\n",
       "      <td>0.402890</td>\n",
       "      <td>0.096628</td>\n",
       "      <td>-0.116460</td>\n",
       "      <td>0.063727</td>\n",
       "      <td>0.089034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061127</td>\n",
       "      <td>0.068978</td>\n",
       "      <td>0.017745</td>\n",
       "      <td>0.046461</td>\n",
       "      <td>-0.015418</td>\n",
       "      <td>-0.101892</td>\n",
       "      <td>Hylidae</td>\n",
       "      <td>Scinax</td>\n",
       "      <td>ScinaxRuber</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7192</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.582557</td>\n",
       "      <td>-0.343237</td>\n",
       "      <td>0.029468</td>\n",
       "      <td>0.064179</td>\n",
       "      <td>0.385596</td>\n",
       "      <td>0.114905</td>\n",
       "      <td>-0.103317</td>\n",
       "      <td>0.070370</td>\n",
       "      <td>0.081317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082474</td>\n",
       "      <td>0.077771</td>\n",
       "      <td>-0.009688</td>\n",
       "      <td>0.027834</td>\n",
       "      <td>-0.000531</td>\n",
       "      <td>-0.080425</td>\n",
       "      <td>Hylidae</td>\n",
       "      <td>Scinax</td>\n",
       "      <td>ScinaxRuber</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7193</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.519497</td>\n",
       "      <td>-0.307553</td>\n",
       "      <td>-0.004922</td>\n",
       "      <td>0.072865</td>\n",
       "      <td>0.377131</td>\n",
       "      <td>0.086866</td>\n",
       "      <td>-0.115799</td>\n",
       "      <td>0.056979</td>\n",
       "      <td>0.089316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051796</td>\n",
       "      <td>0.069073</td>\n",
       "      <td>0.017963</td>\n",
       "      <td>0.041803</td>\n",
       "      <td>-0.027911</td>\n",
       "      <td>-0.096895</td>\n",
       "      <td>Hylidae</td>\n",
       "      <td>Scinax</td>\n",
       "      <td>ScinaxRuber</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7194</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.508833</td>\n",
       "      <td>-0.324106</td>\n",
       "      <td>0.062068</td>\n",
       "      <td>0.078211</td>\n",
       "      <td>0.397188</td>\n",
       "      <td>0.094596</td>\n",
       "      <td>-0.117672</td>\n",
       "      <td>0.058874</td>\n",
       "      <td>0.076180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061455</td>\n",
       "      <td>0.072983</td>\n",
       "      <td>-0.003980</td>\n",
       "      <td>0.031560</td>\n",
       "      <td>-0.029355</td>\n",
       "      <td>-0.087910</td>\n",
       "      <td>Hylidae</td>\n",
       "      <td>Scinax</td>\n",
       "      <td>ScinaxRuber</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7195 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MFCCs_ 1  MFCCs_ 2  MFCCs_ 3  MFCCs_ 4  MFCCs_ 5  MFCCs_ 6  MFCCs_ 7  \\\n",
       "0          1.0  0.152936 -0.105586  0.200722  0.317201  0.260764  0.100945   \n",
       "1          1.0  0.171534 -0.098975  0.268425  0.338672  0.268353  0.060835   \n",
       "2          1.0  0.152317 -0.082973  0.287128  0.276014  0.189867  0.008714   \n",
       "3          1.0  0.224392  0.118985  0.329432  0.372088  0.361005  0.015501   \n",
       "4          1.0  0.087817 -0.068345  0.306967  0.330923  0.249144  0.006884   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7190       1.0 -0.554504 -0.337717  0.035533  0.034511  0.443451  0.093889   \n",
       "7191       1.0 -0.517273 -0.370574  0.030673  0.068097  0.402890  0.096628   \n",
       "7192       1.0 -0.582557 -0.343237  0.029468  0.064179  0.385596  0.114905   \n",
       "7193       1.0 -0.519497 -0.307553 -0.004922  0.072865  0.377131  0.086866   \n",
       "7194       1.0 -0.508833 -0.324106  0.062068  0.078211  0.397188  0.094596   \n",
       "\n",
       "      MFCCs_ 8  MFCCs_ 9  MFCCs_10  ...  MFCCs_17  MFCCs_18  MFCCs_19  \\\n",
       "0    -0.150063 -0.171128  0.124676  ... -0.108351 -0.077623 -0.009568   \n",
       "1    -0.222475 -0.207693  0.170883  ... -0.090974 -0.056510 -0.035303   \n",
       "2    -0.242234 -0.219153  0.232538  ... -0.050691 -0.023590 -0.066722   \n",
       "3    -0.194347 -0.098181  0.270375  ... -0.136009 -0.177037 -0.130498   \n",
       "4    -0.265423 -0.172700  0.266434  ... -0.048885 -0.053074 -0.088550   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "7190 -0.100753  0.037087  0.081075  ...  0.069430  0.071001  0.021591   \n",
       "7191 -0.116460  0.063727  0.089034  ...  0.061127  0.068978  0.017745   \n",
       "7192 -0.103317  0.070370  0.081317  ...  0.082474  0.077771 -0.009688   \n",
       "7193 -0.115799  0.056979  0.089316  ...  0.051796  0.069073  0.017963   \n",
       "7194 -0.117672  0.058874  0.076180  ...  0.061455  0.072983 -0.003980   \n",
       "\n",
       "      MFCCs_20  MFCCs_21  MFCCs_22           Family      Genus  \\\n",
       "0     0.057684  0.118680  0.014038  Leptodactylidae  Adenomera   \n",
       "1     0.020140  0.082263  0.029056  Leptodactylidae  Adenomera   \n",
       "2    -0.025083  0.099108  0.077162  Leptodactylidae  Adenomera   \n",
       "3    -0.054766 -0.018691  0.023954  Leptodactylidae  Adenomera   \n",
       "4    -0.031346  0.108610  0.079244  Leptodactylidae  Adenomera   \n",
       "...        ...       ...       ...              ...        ...   \n",
       "7190  0.052449 -0.021860 -0.079860          Hylidae     Scinax   \n",
       "7191  0.046461 -0.015418 -0.101892          Hylidae     Scinax   \n",
       "7192  0.027834 -0.000531 -0.080425          Hylidae     Scinax   \n",
       "7193  0.041803 -0.027911 -0.096895          Hylidae     Scinax   \n",
       "7194  0.031560 -0.029355 -0.087910          Hylidae     Scinax   \n",
       "\n",
       "             Species  RecordID  \n",
       "0     AdenomeraAndre         1  \n",
       "1     AdenomeraAndre         1  \n",
       "2     AdenomeraAndre         1  \n",
       "3     AdenomeraAndre         1  \n",
       "4     AdenomeraAndre         1  \n",
       "...              ...       ...  \n",
       "7190     ScinaxRuber        60  \n",
       "7191     ScinaxRuber        60  \n",
       "7192     ScinaxRuber        60  \n",
       "7193     ScinaxRuber        60  \n",
       "7194     ScinaxRuber        60  \n",
       "\n",
       "[7195 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frogs = pd.read_csv('../data/Frogs_MFCCs.csv')\n",
    "frogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "107b1be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types = []\n",
    "for i in frogs.columns:\n",
    "    types.append(type(frogs[i][2]))\n",
    "pd.unique(types)\n",
    "type(frogs['RecordID'][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e06e7dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features (X) and labels (y)\n",
    "X = frogs.iloc[:, :-4]\n",
    "y = frogs[['Family', 'Genus', 'Species']]\n",
    "y_family = frogs['Family']\n",
    "y_genus = frogs['Genus']\n",
    "y_species = frogs['Species']\n",
    "\n",
    "# Split the data into training and testing sets, with 70% for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c39ad1e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Family', 'Genus', 'Species'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53816d47",
   "metadata": {},
   "source": [
    "#### (b) Each instance has three labels: Families, Genus, and Species. Each of the labels has multiple classes. We wish to solve a multi-class and multi-label problem.\n",
    "#### One of the most important approaches to multi-label classification is to train a classifier for each label (binary relevance). We first try this approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a77cdc",
   "metadata": {},
   "source": [
    "#### i. Research exact match and hamming score/ loss methods for evaluating multi-label classification and use them in evaluating the classifiers in this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e19be77a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss: 0.023853635942566002\n",
      "EMR: 0.8628994905048634\n"
     ]
    }
   ],
   "source": [
    "# EMR = (# instances w/ all labels correctly predicted)/ (total # of instances)\n",
    "\n",
    "# Hamming Loss = (# of incorrectly predicted labels)/(total # of labels)\n",
    "\n",
    "# Hamming score = 1-Hamming Loss\n",
    "\n",
    "# Convert the labels to binary format\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_b = mlb.fit_transform(y.iloc[:,:-1].values)\n",
    "y_b = pd.DataFrame(y_b)\n",
    "# Split the data into training and testing sets, with 70% for training\n",
    "X_train, X_test, y_train_b, y_test_b = train_test_split(X, y_b, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a One-vs-Rest SVM classifier\n",
    "clf = OneVsRestClassifier(SVC(kernel='linear'))\n",
    "clf.fit(X_train, y_train_b)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the Hamming Loss\n",
    "hl = hamming_loss(y_test_b, y_pred)\n",
    "print(\"Hamming Loss:\", hl)\n",
    "\n",
    "# Calculate the Exact Match Ratio (EMR)\n",
    "emr = accuracy_score(y_test_b, y_pred)\n",
    "print(\"EMR:\", emr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920d37ab",
   "metadata": {},
   "source": [
    "#### ii. Train a SVM for each of the labels, using Gaussian kernels and one versus all classifiers. Determine the weight of the SVM penalty and the width of the Gaussian Kernel using 10 fold cross validation.[<sup>1</sup>](#fn1) You are welcome to try to solve the problem with both standardized [<sup>2</sup>](#fn2) and raw attributes and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da54e653",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM for label: Family\n",
      "Best parameters: {'C': 10, 'gamma': 0.1}\n",
      "Hamming Loss: 0.007410838351088467\n",
      "Hamming score: 0.9925891616489115\n",
      "EMR: 0.9925891616489115\n",
      "Time taken: 0:05:36.157530\n",
      "Training SVM for label: Genus\n",
      "Best parameters: {'C': 10, 'gamma': 0.1}\n",
      "Hamming Loss: 0.012042612320518759\n",
      "Hamming score: 0.9879573876794813\n",
      "EMR: 0.9879573876794813\n",
      "Time taken: 0:08:43.148509\n",
      "Training SVM for label: Species\n",
      "Best parameters: {'C': 1000, 'gamma': 0.01}\n",
      "Hamming Loss: 0.010653080129689671\n",
      "Hamming score: 0.9893469198703103\n",
      "EMR: 0.9893469198703103\n",
      "Time taken: 0:08:14.075768\n"
     ]
    }
   ],
   "source": [
    "# Try Standardized dataset\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Train an SVM for each label\n",
    "for label in y.columns:\n",
    "    t1 = datetime.now()  # start timer\n",
    "\n",
    "    print(\"Training SVM for label:\", label)\n",
    "\n",
    "    # Define the parameter grid to search over\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1, 10, 100,1000],\n",
    "        'gamma': [0.01, 0.1, 1, 10]\n",
    "    }\n",
    "\n",
    "    # Perform 10-fold cross validation to find the best parameters\n",
    "    grid_search = GridSearchCV(SVC(kernel='rbf', decision_function_shape='ovr'), param_grid, cv=10)\n",
    "    grid_search.fit(X_train_std, y_train[label])\n",
    "\n",
    "    # Print the best parameters and time taken\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "    # Train the SVM with the best parameters on the full training set\n",
    "    best_svc = grid_search.best_estimator_\n",
    "    best_svc.fit(X_train_std, y_train[label])\n",
    "\n",
    "    # Evaluate the SVM on the test set\n",
    "    y_pred = best_svc.predict(X_test_std)\n",
    "    #print(y_pred)\n",
    "    #print(y_test[label])\n",
    "    # Calculate the Hamming Loss\n",
    "    hl = hamming_loss(y_test[label], y_pred)\n",
    "    print(\"Hamming Loss:\", hl)\n",
    "\n",
    "    # Calculate the Hamming Score\n",
    "    print(\"Hamming score:\", 1-hl)\n",
    "\n",
    "    # Calculate the Exact Match Ratio (EMR)\n",
    "    emr = accuracy_score(y_test[label], y_pred)\n",
    "    print(\"EMR:\", emr)\n",
    "\n",
    "    t2 = datetime.now()  # stop timer\n",
    "    print(\"Time taken:\", t2 - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6b2951",
   "metadata": {},
   "source": [
    "### Please ignore the last loop for RecordID, it was included by accident."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006df2a1",
   "metadata": {},
   "source": [
    "#### iii. Repeat 1(b)ii with L1-penalized SVMs.[<sup>3</sup>](#fn3) Remember to standardize[<sup>4</sup>](#fn4) the attributes. Determine the weight of the SVM penalty using 10 fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ebe66ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training L1-penalized SVM for label: Family\n",
      "Best penalty weight: {'C': 1}\n",
      "Hamming Loss: 0.07132931912922649\n",
      "Hamming score: 0.9286706808707735\n",
      "EMR: 0.9286706808707735\n",
      "Time taken: 0:00:43.739338\n",
      "Training L1-penalized SVM for label: Genus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alberli/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/alberli/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best penalty weight: {'C': 10}\n",
      "Hamming Loss: 0.058360352014821676\n",
      "Hamming score: 0.9416396479851783\n",
      "EMR: 0.9416396479851783\n",
      "Time taken: 0:01:12.410216\n",
      "Training L1-penalized SVM for label: Species\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alberli/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/alberli/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best penalty weight: {'C': 10}\n",
      "Hamming Loss: 0.04075961093098657\n",
      "Hamming score: 0.9592403890690134\n",
      "EMR: 0.9592403890690134\n",
      "Time taken: 0:01:14.795236\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Train an L1-penalized SVM for each label\n",
    "for label in y.columns:\n",
    "    t1 = datetime.now()  # start timer\n",
    "\n",
    "    print(\"Training L1-penalized SVM for label:\", label)\n",
    "    \n",
    "    # Define the parameter grid to search over\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1, 10, 100]\n",
    "    }\n",
    "    \n",
    "    # Perform 10-fold cross validation to find the best penalty weight\n",
    "    grid_search = GridSearchCV(LinearSVC(penalty='l1', dual=False, max_iter=10000), param_grid, cv=10)\n",
    "    grid_search.fit(X_train_std, y_train[label])\n",
    "    \n",
    "    # Print the best penalty weight\n",
    "    print(\"Best penalty weight:\", grid_search.best_params_)\n",
    "    \n",
    "    # Train the L1-penalized SVM with the best penalty weight on the full training set\n",
    "    best_svc = grid_search.best_estimator_\n",
    "    best_svc.fit(X_train_std, y_train[label])\n",
    "    \n",
    "    # Evaluate the L1-penalized SVM on the test set\n",
    "    y_pred = best_svc.predict(X_test_std)\n",
    "    # Calculate the Hamming Loss\n",
    "    hl = hamming_loss(y_test[label], y_pred)\n",
    "    print(\"Hamming Loss:\", hl)\n",
    "\n",
    "    # Calculate the Hamming Score\n",
    "    print(\"Hamming score:\", 1-hl)\n",
    "\n",
    "    # Calculate the Exact Match Ratio (EMR)\n",
    "    emr = accuracy_score(y_test[label], y_pred)\n",
    "    print(\"EMR:\", emr)\n",
    "\n",
    "    t2 = datetime.now()  # stop timer\n",
    "    print(\"Time taken:\", t2 - t1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6ba72a",
   "metadata": {},
   "source": [
    "#### iv. Repeat 1(b)iii by using SMOTE or any other method you know to remedy class imbalance. Report your conclusions about the classifiers you trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "636470c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM for label: Family\n",
      "Best parameters: {'svm__C': 100, 'svm__gamma': 0.1}\n",
      "Hamming Loss: 0.006947660954145438\n",
      "Hamming score: 0.9930523390458545\n",
      "EMR: 0.9930523390458545\n",
      "Time taken: 0:10:57.928253\n",
      "Training SVM for label: Genus\n",
      "Best parameters: {'svm__C': 10, 'svm__gamma': 0.1}\n",
      "Hamming Loss: 0.010653080129689671\n",
      "Hamming score: 0.9893469198703103\n",
      "EMR: 0.9893469198703103\n",
      "Time taken: 0:39:48.447235\n",
      "Training SVM for label: Species\n",
      "Best parameters: {'svm__C': 10, 'svm__gamma': 0.01}\n",
      "Hamming Loss: 0.012042612320518759\n",
      "Hamming score: 0.9879573876794813\n",
      "EMR: 0.9879573876794813\n",
      "Time taken: 0:47:34.207667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__gamma': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "\n",
    "# Train an SVM for each label using SMOTE\n",
    "for label in y.columns:\n",
    "    t1 = datetime.now()  # start timer\n",
    "\n",
    "    print(\"Training SVM for label:\", label)\n",
    "    \n",
    "    # Define the pipeline to include SMOTE and SVM\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', SMOTE(sampling_strategy='auto', random_state=42)),\n",
    "        ('svm', SVC(kernel='rbf', decision_function_shape='ovr'))\n",
    "    ])\n",
    "    \n",
    "    # Perform 10-fold cross validation to find the best parameters\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=10)\n",
    "    grid_search.fit(X_train_std, y_train[label])\n",
    "    \n",
    "    # Print the best parameters\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    \n",
    "    # Train the SVM with the best parameters on the full training set\n",
    "    best_svc = grid_search.best_estimator_\n",
    "    best_svc.fit(X_train_std, y_train[label])\n",
    "    \n",
    "    # Evaluate the SVM on the test set\n",
    "    y_pred = best_svc.predict(X_test_std)\n",
    "    # Calculate the Hamming Loss\n",
    "    hl = hamming_loss(y_test[label], y_pred)\n",
    "    print(\"Hamming Loss:\", hl)\n",
    "\n",
    "    # Calculate the Hamming Score\n",
    "    print(\"Hamming score:\", 1-hl)\n",
    "\n",
    "    # Calculate the Exact Match Ratio (EMR)\n",
    "    emr = accuracy_score(y_test[label], y_pred)\n",
    "    print(\"EMR:\", emr)\n",
    "\n",
    "    t2 = datetime.now()  # stop timer\n",
    "    print(\"Time taken:\", t2 - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b15eb1",
   "metadata": {},
   "source": [
    "#### v. Extra Practice: Study the Classifier Chain method and apply it to the above problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b66b2fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5276b561",
   "metadata": {},
   "source": [
    "#### vi. Extra Practice: Research how confusion matrices, precision, recall, ROC, and AUC are defined for multi-label classification and compute them for the classifiers you trained in above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1150637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a4ed90b",
   "metadata": {},
   "source": [
    "### 2. K-Means Clustering on a Multi-Class and Multi-Label Data Set Monte-Carlo Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a404ba",
   "metadata": {},
   "source": [
    "#### Perform the following procedures 50 times, and report the average and standard deviation of the 50 Hamming Distances that you calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b64241a",
   "metadata": {},
   "source": [
    "#### (a) Use k-means clustering on the whole Anuran Calls (MFCCs) Data Set (do not split the data into train and test, as we are not performing supervised learning in this exercise). Choose $k \\in \\{1,2,......,50\\}$ automatically based on one of the methods provided in the slides (CH or Gap Statistics or scree plots or Silhouettes) or any other method you know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0998d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gap-stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c49b3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration :  0\n",
      "optimal_k is: 4\n",
      "0:00:41.797671\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.998264\n",
      "Iteration :  1\n",
      "optimal_k is: 4\n",
      "0:00:39.807788\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.207810\n",
      "Iteration :  2\n",
      "optimal_k is: 4\n",
      "0:00:44.763541\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:02.543177\n",
      "Iteration :  3\n",
      "optimal_k is: 4\n",
      "0:00:36.192246\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.397213\n",
      "Iteration :  4\n",
      "optimal_k is: 4\n",
      "0:00:48.036231\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.957343\n",
      "Iteration :  5\n",
      "optimal_k is: 4\n",
      "0:00:39.811648\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.118255\n",
      "Iteration :  6\n",
      "optimal_k is: 4\n",
      "0:00:49.427445\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.186773\n",
      "Iteration :  7\n",
      "optimal_k is: 4\n",
      "0:00:31.968724\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.098594\n",
      "Iteration :  8\n",
      "optimal_k is: 4\n",
      "0:00:30.201725\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.323409\n",
      "Iteration :  9\n",
      "optimal_k is: 4\n",
      "0:00:32.692727\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.905511\n",
      "Iteration :  10\n",
      "optimal_k is: 4\n",
      "0:00:36.523914\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.854129\n",
      "Iteration :  11\n",
      "optimal_k is: 4\n",
      "0:00:43.792893\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:02.623238\n",
      "Iteration :  12\n",
      "optimal_k is: 4\n",
      "0:00:45.733690\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:11.138991\n",
      "Iteration :  13\n",
      "optimal_k is: 4\n",
      "0:00:45.412960\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.062261\n",
      "Iteration :  14\n",
      "optimal_k is: 4\n",
      "0:00:45.906822\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.973735\n",
      "Iteration :  15\n",
      "optimal_k is: 4\n",
      "0:00:41.433882\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.785621\n",
      "Iteration :  16\n",
      "optimal_k is: 4\n",
      "0:00:46.650755\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.014958\n",
      "Iteration :  17\n",
      "optimal_k is: 4\n",
      "0:00:36.931097\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.164616\n",
      "Iteration :  18\n",
      "optimal_k is: 4\n",
      "0:00:51.245084\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.257930\n",
      "Iteration :  19\n",
      "optimal_k is: 4\n",
      "0:00:41.976367\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.166201\n",
      "Iteration :  20\n",
      "optimal_k is: 4\n",
      "0:00:50.778980\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.482562\n",
      "Iteration :  21\n",
      "optimal_k is: 4\n",
      "0:00:46.541517\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.191891\n",
      "Iteration :  22\n",
      "optimal_k is: 4\n",
      "0:00:44.651498\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.004413\n",
      "Iteration :  23\n",
      "optimal_k is: 4\n",
      "0:01:01.798667\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.131587\n",
      "Iteration :  24\n",
      "optimal_k is: 4\n",
      "0:00:44.484220\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:02.030205\n",
      "Iteration :  25\n",
      "optimal_k is: 4\n",
      "0:00:54.591801\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.027336\n",
      "Iteration :  26\n",
      "optimal_k is: 4\n",
      "0:00:44.877746\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:02.897131\n",
      "Iteration :  27\n",
      "optimal_k is: 4\n",
      "0:00:43.453239\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.211115\n",
      "Iteration :  28\n",
      "optimal_k is: 5\n",
      "0:00:40.439821\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.061741\n",
      "Iteration :  29\n",
      "optimal_k is: 4\n",
      "0:00:35.102489\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.690915\n",
      "Iteration :  30\n",
      "optimal_k is: 4\n",
      "0:00:47.869642\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:02.957447\n",
      "Iteration :  31\n",
      "optimal_k is: 4\n",
      "0:00:51.160731\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:08.114504\n",
      "Iteration :  32\n",
      "optimal_k is: 4\n",
      "0:01:58.153963\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.097156\n",
      "Iteration :  33\n",
      "optimal_k is: 4\n",
      "0:00:41.873194\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.441129\n",
      "Iteration :  34\n",
      "optimal_k is: 4\n",
      "0:00:49.571922\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.913099\n",
      "Iteration :  35\n",
      "optimal_k is: 4\n",
      "0:00:40.133129\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:02.112467\n",
      "Iteration :  36\n",
      "optimal_k is: 4\n",
      "0:00:49.914275\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.031734\n",
      "Iteration :  37\n",
      "optimal_k is: 4\n",
      "0:00:47.404806\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:05.451492\n",
      "Iteration :  38\n",
      "optimal_k is: 4\n",
      "0:00:41.837681\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.065926\n",
      "Iteration :  39\n",
      "optimal_k is: 4\n",
      "0:00:39.705887\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.935306\n",
      "Iteration :  40\n",
      "optimal_k is: 4\n",
      "0:00:56.606554\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.975337\n",
      "Iteration :  41\n",
      "optimal_k is: 4\n",
      "0:00:45.153758\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.894794\n",
      "Iteration :  42\n",
      "optimal_k is: 4\n",
      "0:00:40.180382\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:02.787213\n",
      "Iteration :  43\n",
      "optimal_k is: 4\n",
      "0:00:44.938067\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.205024\n",
      "Iteration :  44\n",
      "optimal_k is: 4\n",
      "0:00:46.161807\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.027336\n",
      "Iteration :  45\n",
      "optimal_k is: 4\n",
      "0:00:44.260116\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:03.155975\n",
      "Iteration :  46\n",
      "optimal_k is: 4\n",
      "0:00:43.410740\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.975615\n",
      "Iteration :  47\n",
      "optimal_k is: 4\n",
      "0:00:46.741911\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.358751\n",
      "Iteration :  48\n",
      "optimal_k is: 4\n",
      "0:00:40.665602\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.140334\n",
      "Iteration :  49\n",
      "optimal_k is: 4\n",
      "0:01:15.380093\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:01.134554\n",
      "Label: Family\n",
      "Mean Hamming distance: 1.0\n",
      "Standard deviation of Hamming distance: 0.0\n",
      "Label: Genus\n",
      "Mean Hamming distance: 1.0\n",
      "Standard deviation of Hamming distance: 0.0\n",
      "Label: Species\n",
      "Mean Hamming distance: 1.0\n",
      "Standard deviation of Hamming distance: 0.0\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Perform the procedure 50 times\n",
    "n_iter = 50\n",
    "hamming_distances = {label: [] for label in y.columns}\n",
    "for i in range(n_iter):\n",
    "    # Find optimal number of clusters using Silhouette score\n",
    "    silh_avg = {}\n",
    "    t1 = datetime.now()  # start timer\n",
    "\n",
    "    for k in range(2,21):\n",
    "        rand_value=random.randint(0, 100)\n",
    "        k_means = KMeans(n_clusters=k,init='k-means++',random_state=rand_value).fit(X)\n",
    "        labels = k_means.labels_\n",
    "        silh_avg.update({k:(metrics.silhouette_score(X, labels))})\n",
    "   \n",
    "    print(\"Iteration : \",i)\n",
    "\n",
    "    # Get optimal k and build K-means clustering with it\n",
    "    optimal_k = max(silh_avg,key=silh_avg.get)\n",
    "    rand_value=random.randint(0, 900)\n",
    "    print('optimal_k is:',optimal_k)\n",
    "    t2 = datetime.now()  \n",
    "    print(t2-t1)\n",
    "    \n",
    "    t3 = datetime.now()  # start timer\n",
    "\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=rand_value)\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    # Predict target variables for all possible triplets in each cluster\n",
    "    for j in range(optimal_k):\n",
    "        cluster_labels = kmeans.labels_ == j\n",
    "        cluster_y = y[cluster_labels]\n",
    "        triplet_list = list(itertools.product([0, 1], repeat=3))\n",
    "        print(len(triplet_list))\n",
    "        for triplet in triplet_list:\n",
    "            predicted_y = np.array([triplet for _ in range(sum(cluster_labels))])\n",
    "            for label in y.columns:\n",
    "                hamming_dist = hamming_loss(cluster_y[label].astype(str), predicted_y[:, y.columns.get_loc(label)].astype(str))\n",
    "                hamming_distances[label].append(hamming_dist)            \n",
    "#             hamming_distances[y.columns[0]].append(hamming_dist[0])\n",
    "#             hamming_distances[y.columns[1]].append(hamming_dist[1])\n",
    "#             hamming_distances[y.columns[2]].append(hamming_dist[2])\n",
    "    t4 = datetime.now()  # start timer\n",
    "    print(t4-t3)\n",
    "\n",
    "\n",
    "            # Calculate the mean and standard deviation of the Hamming distances for each column\n",
    "mean_dict = {}\n",
    "std_dict = {}\n",
    "for label in y.columns:\n",
    "    mean_dist = np.mean(hamming_distances[label])\n",
    "    std_dist = np.std(hamming_distances[label])\n",
    "    mean_dict[label] = mean_dist\n",
    "    std_dict[label] = std_dist\n",
    "    print(\"Label:\", label)\n",
    "    print(\"Mean Hamming distance:\", mean_dist)\n",
    "    print(\"Standard deviation of Hamming distance:\", std_dist)\n",
    "\n",
    "# Calculate overall mean and standard deviation of Hamming distances for each label\n",
    "mean_hamming_distances = sum(mean_dict.values())/len(mean_dict)\n",
    "sd_hamming_distances = sum(std_dict.values())/len(std_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88f4ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Hamming distance: \", 1-mean_hamming_distances)\n",
    "print(\"Standard deviation of Hamming distance: \", sd_hamming_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca715331",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The optimal K is: 4\n",
      "Iteration 1 - Hamming distance: 0.665, Hamming loss: 0.222, Hamming score: 0.778\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bl/cyzb7l7939lg7l3437y92vc00000gn/T/ipykernel_9262/3975942825.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhamming_distances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mrun_monte_carlo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/bl/cyzb7l7939lg7l3437y92vc00000gn/T/ipykernel_9262/3975942825.py\u001b[0m in \u001b[0;36mrun_monte_carlo\u001b[0;34m(df, iterations)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mhamming_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0moptimal_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_optimal_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mclusterer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimal_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mcluster_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclusterer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bl/cyzb7l7939lg7l3437y92vc00000gn/T/ipykernel_9262/3975942825.py\u001b[0m in \u001b[0;36mget_optimal_k\u001b[0;34m(num_clusters, X, random_state)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_clusters_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mclusterer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mcluster_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclusterer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0msilhouette_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msilhouette_avg\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mIndex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m \u001b[0mbelongs\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m         \"\"\"\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0;31m# Initialize centers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m             centers_init = self._init_centroids(\n\u001b[0m\u001b[1;32m   1180\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_squared_norms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_squared_norms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_init_centroids\u001b[0;34m(self, X, x_squared_norms, init, random_state, init_size)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"k-means++\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m             centers, _ = _kmeans_plusplus(\n\u001b[0m\u001b[1;32m   1091\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m                 \u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_kmeans_plusplus\u001b[0;34m(X, n_clusters, x_squared_norms, random_state, n_local_trials)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;31m# to the squared distance to the closest existing center\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mrand_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_local_trials\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcurrent_pot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mcandidate_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstable_cumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosest_dist_sq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrand_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;31m# XXX: numerical imprecision can result in a candidate_id out of range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosest_dist_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcandidate_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36mstable_cumsum\u001b[0;34m(arr, axis, rtol, atol)\u001b[0m\n\u001b[1;32m   1079\u001b[0m     \u001b[0mexpected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     if not np.all(\n\u001b[0;32m-> 1081\u001b[0;31m         np.isclose(\n\u001b[0m\u001b[1;32m   1082\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequal_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         )\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36misclose\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36misclose\u001b[0;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[1;32m   2356\u001b[0m     \u001b[0myfin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxfin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwithin_tol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2359\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2360\u001b[0m         \u001b[0mfinite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxfin\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0myfin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mwithin_tol\u001b[0;34m(x, y, atol, rtol)\u001b[0m\n\u001b[1;32m   2336\u001b[0m     \"\"\"\n\u001b[1;32m   2337\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithin_tol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2338\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mless_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrtol\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_ufunc_config.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moldstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseterr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_Unspecified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moldcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseterrcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/_ufunc_config.py\u001b[0m in \u001b[0;36mseterr\u001b[0;34m(all, divide, over, under, invalid)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mpyvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaskvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mumath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseterrobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def get_optimal_k(num_clusters, X, random_state):\n",
    "    optimalK, max_score = 2, 0\n",
    "    num_clusters_new = num_clusters + 1\n",
    "    for n in range(2, num_clusters_new):\n",
    "        clusterer = KMeans(n_clusters=n, random_state=random_state)\n",
    "        cluster_labels = clusterer.fit_predict(X)\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        if silhouette_avg > max_score:\n",
    "            optimalK = n\n",
    "            max_score = silhouette_avg\n",
    "    print(f\"\\nThe optimal K is: {optimalK}\")\n",
    "    return optimalK\n",
    "\n",
    "def MajorityLabels(optimalK, cluster_labels, Y):\n",
    "    cluster_major = pd.DataFrame(columns=Y.columns)\n",
    "    for c in range(optimalK):\n",
    "        cluster_idx, = np.where(cluster_labels == c)\n",
    "        cluster_samples = Y.iloc[cluster_idx, :]\n",
    "        row = []\n",
    "        for label in Y.columns:\n",
    "            cur_major = cluster_samples.loc[:, label].value_counts().index[0]\n",
    "            row.append(cur_major)\n",
    "        cluster_major.loc[c] = row\n",
    "    return cluster_major\n",
    "\n",
    "def calculation(cluster_major, cluster_labels, Y):\n",
    "    missclf_labels = 0\n",
    "    for c in range(len(cluster_major)):\n",
    "        cluster_idx, = np.where(cluster_labels == c)\n",
    "        for label in Y.loc[cluster_idx].values:\n",
    "            miss = (label != cluster_major.loc[c].values)\n",
    "            missclf_labels += np.sum(miss)\n",
    "    hamming_dist = missclf_labels / Y.shape[0]\n",
    "    hamming_loss = missclf_labels / (Y.shape[0] * Y.shape[1])\n",
    "    return hamming_dist, hamming_loss\n",
    "\n",
    "def run_monte_carlo(df, iterations):\n",
    "    hamming_distances = []\n",
    "    hamming_losses = []\n",
    "    hamming_scores = []\n",
    "    for i in range(iterations):\n",
    "        optimal_k = get_optimal_k(50, df.iloc[:, :-4], i)\n",
    "        clusterer = KMeans(n_clusters=optimal_k, random_state=i)\n",
    "        cluster_labels = clusterer.fit_predict(df.iloc[:, :-4])\n",
    "        cluster_major = MajorityLabels(optimal_k, cluster_labels, df.iloc[:, -4:-1])\n",
    "        hamming_distance, hamming_loss_val = calculation(cluster_major, cluster_labels, df.iloc[:, -4:-1])\n",
    "        hamming_score = 1 - hamming_loss_val\n",
    "        hamming_distances.append(hamming_distance)\n",
    "        hamming_losses.append(hamming_loss_val)\n",
    "        hamming_scores.append(hamming_score)\n",
    "        print(f\"Iteration {i+1} - Hamming distance: {hamming_distance:.3f}, Hamming loss: {hamming_loss_val:.3f}, Hamming score: {hamming_score:.3f}\")\n",
    "    \n",
    "    # calculate hamming score std for all possible triplets\n",
    "    hamming_score_std = []\n",
    "    for i in range(len(hamming_scores)-2):\n",
    "        for j in range(i+1, len(hamming_scores)-1):\n",
    "            for k in range(j+1, len(hamming_scores)):\n",
    "                triplet = [hamming_scores[i], hamming_scores[j], hamming_scores[k]]\n",
    "                hamming_score_std.append(np.std(triplet))\n",
    "    print(f\"Hamming score std for all possible triplets: {np.mean(hamming_score_std):.3f}\")\n",
    "    \n",
    "    return hamming_distances\n",
    "run_monte_carlo(frogs,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa92ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "lst = [0.6653231410701876, 0.66726893676164, 0.7357887421820709, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.6674079221681724, 0.66726893676164, 0.6674079221681724, 0.66726893676164, 0.7011813759555247, 0.66726893676164, 0.6653231410701876, 0.7021542738012508, 0.66726893676164, 0.66726893676164, 0.6674079221681724, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.6674079221681724, 0.66726893676164, 0.6664350243224462, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.5581653926337734, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.6653231410701876, 0.66726893676164, 0.66726893676164, 0.8401667824878388, 0.66726893676164]\n",
    "\n",
    "mean_value = statistics.mean(lst)\n",
    "print(mean_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba18aab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "majority_triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d374a0d",
   "metadata": {},
   "source": [
    "#### (b) In each cluster, determine which family is the majority by reading the true labels. Repeat for genus and species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cab129",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Perform the procedure 50 times\n",
    "n_iter = 50\n",
    "sil_scores = []\n",
    "majority_triplets = []\n",
    "hamming_distances = {label: [] for label in y.columns}\n",
    "for i in range(n_iter):\n",
    "    # Find optimal number of clusters using Silhouette score\n",
    "    \n",
    "\n",
    "    for k in range(2,21):\n",
    "        rand_value=random.randint(0, 100)\n",
    "        k_means = KMeans(n_clusters=k,init='k-means++',random_state=rand_value).fit(X)\n",
    "        labels = k_means.labels_\n",
    "        silh_avg.update({k:(metrics.silhouette_score(X, labels))})\n",
    "   \n",
    "    print(\"Iteration : \",i)\n",
    "    #print(\"Average Silhoutte score values : \",silh_avg)\n",
    "\n",
    "    # Get optimal k and build K-means clustering with it\n",
    "\n",
    "\n",
    "    optimal_k = max(silh_avg,key=silh_avg.get)\n",
    "    \n",
    "    rand_value=random.randint(0, 900)\n",
    "    print('optimal_k is:',optimal_k)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=rand_value)\n",
    "    kmeans.fit(X)\n",
    "    \n",
    "\n",
    "    # Determine majority triplet for each cluster\n",
    "    for j in range(optimal_k):\n",
    "        cluster_labels = kmeans.labels_ == j\n",
    "        cluster_y = y[cluster_labels]\n",
    "        majority_triplet = cluster_y.mode().iloc[0].values\n",
    "        majority_triplets.append(majority_triplet)\n",
    "    print(len(majority_triplets))\n",
    "\n",
    "    # Predict target variables using majority triplet\n",
    "    predicted_y = np.array([majority_triplets[label] for label in kmeans.labels_])\n",
    "    # Calculate Hamming distance for each column separately\n",
    "    for label in y.columns:\n",
    "        hamming_dist = hamming_loss(y[label], predicted_y[:, y.columns.get_loc(label)])\n",
    "        hamming_distances[label].append(hamming_dist)\n",
    "# Calculate the mean and standard deviation of the Hamming distances for each column\n",
    "mean_dict = {}\n",
    "std_dict = {}\n",
    "for label in y.columns:\n",
    "    mean_dist = np.mean(hamming_distances[label])\n",
    "    std_dist = np.std(hamming_distances[label])\n",
    "    mean_dict[label] = mean_dist\n",
    "    std_dict[label] = std_dist\n",
    "    print(\"Label:\", label)\n",
    "    print(\"Mean Hamming distance:\", mean_dist)\n",
    "    print(\"Standard deviation of Hamming distance:\", std_dist)\n",
    "    \n",
    "# Calculate overall mean and standard deviation of Hamming distances for each label\n",
    "mean_hamming_distances = sum(mean_dict.values())/len(mean_dict)\n",
    "sd_hamming_distances = sum(std_dict.values())/len(std_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a12b01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8421c9bf",
   "metadata": {},
   "source": [
    "#### (c) Now for each cluster you have a majority label triplet (family, genus, species). Calculate the average Hamming distance, Hamming score, and Hamming loss[<sup>5</sup>](#fn5) between the true labels and the labels assigned by clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3d88c9",
   "metadata": {},
   "source": [
    "I have the answer above the majority_triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5bf241",
   "metadata": {},
   "source": [
    "### 3. ISLR 12.6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533a6589",
   "metadata": {},
   "source": [
    "#### a) draw a complete dandrogram based on the dissimilarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ffe296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the distance matrix\n",
    "dis_matrix = np.array([[0, 0.3, 0.4, 0.7],\n",
    "                  [0.3, 0, 0.5, 0.8],\n",
    "                  [0.4, 0.5, 0.0, 0.45],\n",
    "                  [0.7, 0.8, 0.45, 0.0]])\n",
    "                  \n",
    "# Compute hierarchical clustering and plot the dendrogram\n",
    "Z = linkage(dis_matrix, method='complete')\n",
    "fig = plt.figure(figsize=(5,3))\n",
    "dn = dendrogram(Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece896c1",
   "metadata": {},
   "source": [
    "#### b) Repeat (a), this time using simple linkage clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1874ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute hierarchical clustering and plot the dendrogram\n",
    "linkage_matrix = linkage(dis_matrix, method='single')\n",
    "dendrogram(linkage_matrix, color_threshold=0.5, labels=['1', '2', '3', '4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5745a331",
   "metadata": {},
   "source": [
    "#### c) Suppose that we cut the dendrogram obtained in (a) such that two clusters result. Which observations are in each cluster ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a84f645",
   "metadata": {},
   "source": [
    "### <span style='color:blue '>  In this case, we have clusters (1,2) and (3,4).<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a7d4c9",
   "metadata": {},
   "source": [
    "#### d) Suppose that we cut the dendrogram obtained in (b) such that two clusters result. Which observations are in each cluster ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb70469",
   "metadata": {},
   "source": [
    "### <span style='color:blue '>  In this case, we have clusters ((1,2),3) and (4).<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9b406e",
   "metadata": {},
   "source": [
    "#### e) Draw a dendrogram that is equivalent to the dendrogram in (a), for which two or more of the leaves are repositioned, but for which the meaning of the dendrogram is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6943f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linkage(dis_matrix, method='complete')\n",
    "plt.figure(figsize=(8, 6))\n",
    "dendrogram(Z, labels=[1,0,3,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371a71e7",
   "metadata": {},
   "source": [
    "##### <span style='color:green '> Footnote section <span>\n",
    "\n",
    "<span id=\"fn1\">1.How to choose parameter ranges for SVMs? One can use wide ranges for the parameters and a  fine grid (e.g. 1000 points) for cross validation; however,this method may be computationally expensive. An alternative way is to train the SVM with very large and very small parameters on the whole training data and find very large and very small parameters for which the training accuracy is not below a threshold (e.g.,70%). Then one can select a  fixed number of parameters (e.g., 20) between those points for cross validation. For the penalty parameter, usually one has to consider increments in log( $\\lambda$). For example, if one found that the accuracy of a support vector machine will not be below 70% for $\\lambda$ = $10^{-3}$ and $\\lambda$  = $10^{6}$, one has to choose log($\\lambda$ ) $ \\in \\{-3,-2,......,4,5,6\\}$.   \n",
    "    For the Gaussian Kernel parameter, one usually chooses linear increments, e.g. $\\sigma \\in\\{.1,.2,....,2\\}$. When both $\\sigma$  and $\\lambda$ are to be chosen using cross-validation, combinations of very\n",
    "small and very large $\\sigma$'s  and $\\lambda$'s that keep the accuracy above a threshold (e.g.70%) can be used to determine the ranges for   $\\sigma$  and $\\lambda$. Please note that these are very rough rules of thumb, not general procedures. <span>  \n",
    "<span id=\"fn2\"> 2. It seems that the data are already normalized. <span>  \n",
    "<span id=\"fn3\"> 3. The convention is to use L1 penalty with linear kernel.<span>  \n",
    "<span id=\"fn4\"> 4. It seems that the data are already normalized. <span>   \n",
    "<span id=\"fn5\"> 5.Research what these scores are. For example, see the paper A Literature Survey on Algorithms for Multi-label Learning, by Mohammad Sorower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f62c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
